# -*- coding: utf-8 -*-
"""Therm_UNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V7uuS3JbE0Nianw8ySQ619r2zV66qeqG
"""

import os
import numpy as np # linear algebra
import time
from contextlib import contextmanager # timer
from functools import partial
import glob
# import seaborn as sns

import matplotlib.pylab as plt

# from skimage.transform import rescale, resize
import torch
import torch.nn as nn
#from torch.utils import data
from torch.utils.data import DataLoader, Dataset
import albumentations as A  #####
from tqdm import tqdm

import torch.nn.functional as F
import argparse
import math
import os.path
import shutil

import scipy.io
import torch.nn.parallel

from torch.autograd import Variable
import torch.optim

from PIL import Image
import random
import torchvision.transforms as transforms

if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

# transform_train = A.Compose([
#     A.Rotate(limit=45, p=0.5),
#     A.HorizontalFlip(p=0.5),
#     A.Flip(p=0.5),
#     A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.2),
#     A.RandomCrop(width=200, height=200, p=0.5),
#     transforms.Resize((224, 224)),
#     transforms.ToTensor(),
#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
# ])

transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

albumentations_transform = A.Compose([
    A.Rotate(limit=45, p=0.5),
    A.HorizontalFlip(p=0.5),
    A.Flip(p=0.5),
    #A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.2),
    #A.RandomCrop(width=200, height=200, p=0.5),
], additional_targets={"image4": "image", "image3": "image", "image2": "image"})

transform_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

def create_image_pairs(folder_path):
    image_pairs_train = []
    image_pairs_test = []
    all_files = sorted(os.listdir(folder_path))  # Ensure the files are sorted
    
    current_prefix = None
    current_series = []
    
    # Loop over the files and create pairs
    for file in all_files:
        if file.endswith(('.jpg', '.png', '.jpeg')):  # Ensure it's an image file
            prefix, _ = file.split('_', 1)
            
            if current_prefix is None:
                current_prefix = prefix
                
            if prefix == current_prefix:
                current_series.append(file)
            else:
                # Create pairs for the current series
                for i in range(len(current_series) - 1):
                    if (int(current_prefix) % 10 == 1):
                        image_pairs_test.append((current_series[i], current_series[i+1]))
                    else:
                        image_pairs_train.append((current_series[i], current_series[i+1]))
                
                # Reset for the next series
                current_prefix = prefix
                current_series = [file]
    
    # Final pair creation for the last series
    for i in range(len(current_series) - 1):
        image_pairs_train.append((current_series[i], current_series[i+1]))
    
    return image_pairs_train, image_pairs_test

folder_path = '/home/nano01/a/chand133/ThermAI/Dataset/Data2'
image_pairs_train, image_pairs_test = create_image_pairs(folder_path)

class ThermDataset(Dataset): # For training
    def __init__(self, image_pairs, transform=False, study='train'):
        self.image_pairs = image_pairs
        self.transform = transform
        self.study = study

    def __len__(self):
        return len(self.image_pairs)

    def __getitem__(self, idx):
        input_image_filepath, output_image_filepath = self.image_pairs[idx]
        directory, filename = os.path.split(input_image_filepath)
        new_directory = os.path.join(directory, 'Dataset', 'Data')
        input_image_filepath = os.path.join(new_directory, filename)
        directory, filename = os.path.split(output_image_filepath)
        new_directory = os.path.join(directory, 'Dataset', 'Data')
        output_image_filepath = os.path.join(new_directory, filename)
        image_input = Image.open(input_image_filepath).convert("RGB")
        image_output = Image.open(output_image_filepath).convert("RGB")
        image_input = image_input.resize((224,224))
        image_output = image_output.resize((224,224))

        dataset_length = len(self)
        random_idx = random.choice([i for i in range(dataset_length) if i != idx])
        input_image_filepath, output_image_filepath = self.image_pairs[random_idx]
        directory, filename = os.path.split(input_image_filepath)
        new_directory = os.path.join(directory, 'Dataset', 'Data2')
        input_image_filepath = os.path.join(new_directory, filename)
        directory, filename = os.path.split(output_image_filepath)
        new_directory = os.path.join(directory, 'Dataset', 'Data2')
        output_image_filepath = os.path.join(new_directory, filename)
        image_source = Image.open(input_image_filepath).convert("RGB")
        image_target = Image.open(output_image_filepath).convert("RGB")
        image_source = image_source.resize((224,224))
        image_target = image_target.resize((224,224))

        if self.study == 'train':
            image_np = np.array(image_input)
            image_np2 = np.array(image_output)
            image_np3 = np.array(image_source)
            image_np4 = np.array(image_target)
            augmented = albumentations_transform(image=image_np, image2=image_np2, image3=image_np3, image4= image_np4)
            image_np_transformed = augmented['image']
            image_input = Image.fromarray(image_np_transformed)
            image_np_transformed2 = augmented['image2']
            image_output = Image.fromarray(image_np_transformed2)
            image_np_transformed3 = augmented['image3']
            image_source = Image.fromarray(image_np_transformed3)
            image_np_transformed4 = augmented['image4']
            image_target = Image.fromarray(image_np_transformed4)

        if self.transform is not None:
            image_input = self.transform(image_input)
            image_output = self.transform(image_output)
            image_source = self.transform(image_source)
            image_target = self.transform(image_target)

        return image_input, image_output, image_source, image_target, input_image_filepath, output_image_filepath

class ThermDataset2(Dataset): # For inference
    def __init__(self, image_pairs, transform=False, study='train'):
        self.image_pairs = image_pairs
        self.transform = transform
        self.study = study

    def __len__(self):
        return len(self.image_pairs)

    def __getitem__(self, idx):
        input_image_filepath, output_image_filepath = self.image_pairs[idx]
        directory, filename = os.path.split(input_image_filepath)
        new_directory = os.path.join(directory, 'Dataset', 'Data2')
        input_image_filepath = os.path.join(new_directory, filename)
        directory, filename = os.path.split(output_image_filepath)
        new_directory = os.path.join(directory, 'Dataset', 'Data2')
        output_image_filepath = os.path.join(new_directory, filename)
        image_input = Image.open(input_image_filepath).convert("RGB")
        image_output = Image.open(output_image_filepath).convert("RGB")
        image_input = image_input.resize((224,224))
        image_output = image_output.resize((224,224))

        dataset_length = len(self)
        random_idx = random.choice([i for i in range(dataset_length) if i != idx])
        input_image_filepath, output_image_filepath = self.image_pairs[random_idx]
        directory, filename = os.path.split(input_image_filepath)
        new_directory = os.path.join(directory, 'Dataset', 'Data2')
        input_image_filepath = os.path.join(new_directory, filename)
        directory, filename = os.path.split(output_image_filepath)
        new_directory = os.path.join(directory, 'Dataset', 'Data2')
        output_image_filepath = os.path.join(new_directory, filename)
        image_source = Image.open(input_image_filepath).convert("RGB")
        image_target = Image.open(output_image_filepath).convert("RGB")
        image_source = image_source.resize((224,224))
        image_target = image_target.resize((224,224))

        # Both input and output are transformed
        if self.transform is not None:
            image_input = self.transform(image_input)
            image_output = self.transform(image_output)
            image_source = self.transform(image_source)
            image_target = self.transform(image_target)

        return image_input, image_output, image_source, image_target, input_image_filepath, output_image_filepath


def double_conv2d(in_channel, out_channel):
    """
    (convolution => [BN] => ReLU) * 2
    """
    convLayer = nn.Sequential(
        nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1),
        nn.BatchNorm2d(out_channel),
        nn.ReLU(inplace=True),
        nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1),
        nn.BatchNorm2d(out_channel),
        nn.ReLU(inplace=True)
    )
    return convLayer


class PositionalEmbedding(nn.Module):
    def __init__(self, num_channels, height, width):
        super(PositionalEmbedding, self).__init__()
        self.height = height
        self.width = width
        self.num_channels = num_channels
        # Create a learnable positional embedding
        self.embedding = nn.Parameter(torch.randn(1, num_channels, height, width))

    def forward(self, x):
        # Add the positional embedding to the input
        # Resize positional embedding to match the input tensor's spatial dimensions
        embedding_resized = nn.functional.interpolate(self.embedding, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)

        # If the input tensor has more channels (e.g., due to concatenation), repeat the embedding along the channel dimension
        if x.shape[1] != self.num_channels:
            embedding_resized = embedding_resized.repeat(1, x.shape[1] // self.num_channels, 1, 1)

        return x + embedding_resized

class DiffUnet(nn.Module): 
    def __init__(self, height, width):
        super(DiffUnet, self).__init__()
        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)

        # Encoder layers
        self.downConvLayer_1 = double_conv2d(3, 48)
        self.downConvLayer_2 = double_conv2d(48, 96)
        self.downConvLayer_3 = double_conv2d(96, 192)
        self.downConvLayer_4 = double_conv2d(192, 384)
        self.downConvLayer_5 = double_conv2d(384, 768)
        
        # Decoder layers with transposed convolutions
        self.upTransConv1 = nn.ConvTranspose2d(in_channels=768*3, out_channels=384, kernel_size=(2, 2), stride=(2, 2))
        self.upConvLayer_1 = double_conv2d(768 + 384*2, 384)

        self.upTransConv2 = nn.ConvTranspose2d(in_channels=384, out_channels=192, kernel_size=(2, 2), stride=(2, 2))
        self.upConvLayer_2 = double_conv2d(384 + 192*2, 192)

        self.upTransConv3 = nn.ConvTranspose2d(in_channels=192, out_channels=96, kernel_size=(2, 2), stride=(2, 2))
        self.upConvLayer_3 = double_conv2d(192 + 96*2, 96)

        self.upTransConv4 = nn.ConvTranspose2d(in_channels=96, out_channels=48, kernel_size=(2, 2), stride=(2, 2))
        self.upConvLayer_4 = double_conv2d(96 + 48*2, 48)

        self.out = nn.Conv2d(in_channels=48, out_channels=3, kernel_size=(1, 1)) 

        # Positional embedding
        self.positional_embedding = PositionalEmbedding(num_channels=768, height=height//32, width=width//32)

    def encode(self, image):
        # Encoder
        d1_out = self.downConvLayer_1(image)
        d2_out = self.downConvLayer_2(self.max_pool(d1_out))
        d3_out = self.downConvLayer_3(self.max_pool(d2_out))
        d4_out = self.downConvLayer_4(self.max_pool(d3_out))
        d5_out = self.downConvLayer_5(self.max_pool(d4_out))
        return d1_out, d2_out, d3_out, d4_out, d5_out

    def forward(self, image_input, image_source, image_target):
        """
        image_input shape: batch_size, channel, height, width
        image_source shape: batch_size, channel, height, width
        image_target shape: batch_size, channel, height, width
        """

        # Encoding: encode image_input, image_source, and image_target
        d1_in, d2_in, d3_in, d4_in, d5_in = self.encode(image_input)
        d1_source, d2_source, d3_source, d4_source, d5_source = self.encode(image_source)
        d1_target, d2_target, d3_target, d4_target, d5_target = self.encode(image_target)
        

        # Fusion: concatenate the encoded features from the three inputs
        d5_fused = torch.cat([d5_in, d5_source, d5_target], dim=1)

        # Add positional embedding to the fused features
        d5_fused_with_pos = self.positional_embedding(d5_fused)

        """
        Decoder: Decode the fused and positionally embedded features
        """
        up_sampling1 = self.upTransConv1(d5_fused_with_pos)
        u1_out = self.upConvLayer_1(torch.cat([d4_in, d4_source, d4_target, up_sampling1], axis=1))

        up_sampling2 = self.upTransConv2(u1_out)
        u2_out = self.upConvLayer_2(torch.cat([d3_in, d3_source, d3_target, up_sampling2], axis=1))

        up_sampling3 = self.upTransConv3(u2_out)
        u3_out = self.upConvLayer_3(torch.cat([d2_in, d2_source, d2_target, up_sampling3], axis=1))

        up_sampling4 = self.upTransConv4(u3_out)
        u4_out = self.upConvLayer_4(torch.cat([d1_in, d1_source, d1_target, up_sampling4], axis=1))

        # Final output layer
        final_out = self.out(u4_out)

        return final_out


def test(image_path, output_path):
    # image = Image.open(image_path).convert("RGB")
    # output = Image.open(output_path).convert("RGB")

    # Convert images to numpy arrays
    image1_array = np.array(image_path)
    image2_array = np.array(output_path)

    diff = 0
    # Check if the dimensions are the same
    if image1_array.shape == image2_array.shape:
        # Compute the difference
        difference = np.abs(image1_array - image2_array)
        # print(difference.shape)
        diff = np.sum(difference)/(224*224*3)

    return diff

def calculate_iou_for_single_image(input_image, output_image):
  # Calculate the intersection and union of the two images
  intersection = np.logical_and(input_image, output_image)
  union = np.logical_or(input_image, output_image)

  # Calculate the IOU
  iou = np.sum(intersection) / np.sum(union)

  return iou

def get_model_size(model):
    param_size = 0
    param_count = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
        param_count += param.nelement()
    param_size_MB = param_size / (1024 ** 2)
    return param_size_MB, param_count

import numpy as np
from skimage.metrics import structural_similarity as ssim

def calculate_ssim_for_single_image(image_path, output_path):
    # Read and resize images
    image1 = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    image1 = cv2.resize(image1, (224, 224), interpolation=cv2.INTER_AREA)
    image2 = cv2.imread(output_path, cv2.IMREAD_GRAYSCALE)
    image2 = cv2.resize(image2, (224, 224), interpolation=cv2.INTER_AREA)
    
    # Ensure that the images are the same size
    if image1.shape != image2.shape:
        raise ValueError("Input images must have the same dimensions")

    # Calculate SSIM
    ssim_index, _ = ssim(image1, image2, full=True)

    return ssim_index

def calculate_rmse_for_single_image(image_path, output_path):
    # Read and resize images
    image1 = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    image1 = cv2.resize(image1, (224, 224), interpolation=cv2.INTER_AREA)
    image2 = cv2.imread(output_path, cv2.IMREAD_GRAYSCALE)
    image2 = cv2.resize(image2, (224, 224), interpolation=cv2.INTER_AREA)
    
    # Ensure that the images are the same size
    if image1.shape != image2.shape:
        raise ValueError("Input images must have the same dimensions")
    
    # Calculate RMSE
    rmse = np.sqrt(np.mean((image1.astype(np.float32) - image2.astype(np.float32)) ** 2))
    
    return rmse

train_output_image_paths_root = F"/home/nano01/a/chand133/ThermAI/Dataset/Train/Output_Pair_tar/"
test_output_image_paths_root = F"/home/nano01/a/chand133/ThermAI/Dataset/Test/Output_Pair_tar/"


# Load the updated model state dictionary into the model
model_path = "/home/nano01/a/chand133/ThermAI/all_stages_target_early_v2.pth"

if not os.path.exists(train_output_image_paths_root):
    os.makedirs(train_output_image_paths_root)

if not os.path.exists(test_output_image_paths_root):
    os.makedirs(test_output_image_paths_root)


epochs= 100
learning_rate= 1e-6
save_checkpoint= True

amp= False
weight_decay= 1e-8
momentum= 0.999
gradient_clipping= 1.0


# Training
train_dataset = ThermDataset(image_pairs_train, transform_train, study = 'train')
valid_dataset = ThermDataset(image_pairs_test,transform_test, study = 'test')


batch_size = 100
train_loader = DataLoader(
    train_dataset, batch_size=batch_size, shuffle=True
)

valid_loader = DataLoader(
    valid_dataset, batch_size=4, shuffle=False
)

# Inference
train_dataset = ThermDataset2(image_pairs_train,transform_train, study = 'train')
valid_dataset = ThermDataset2(image_pairs_test,transform_test, study = 'test')

batch_size = 1


train_loader2 = DataLoader(
    train_dataset, batch_size=batch_size, shuffle=True
)

valid_loader2 = DataLoader(
    valid_dataset, batch_size=batch_size, shuffle=False
)


def main():
    loss_fn = torch.nn.BCEWithLogitsLoss()
    scaler = torch.cuda.amp.GradScaler()
    model = DiffUnet(224,224)
    
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path))
        print("Model Loaded Successfully!")

    # move initialised model to chosen device
    model = model.to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    patience = 10  # Number of epochs to wait before stopping after last improvement
    min_delta = 0.01  # Minimum change in the monitored quantity to qualify as an improvement
    best_loss = float('inf')
    epochs_without_improvement = 0

    # Training
    for epoch in range(1, epochs+1):
        # print(f"Training epoch {epoch}/{epochs}")
        for i, (input_image, output_image, source_image, target_image, _, _) in enumerate(tqdm(train_loader)):
            input_image = input_image.to(device)
            output_image = output_image.to(device)
            source_image = source_image.to(device)
            target_image = target_image.to(device)
            
            # print(output_image.shape)
            output = model(input_image, source_image, target_image)

            loss = criterion(output, output_image)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    
        print(f"Training epoch {epoch}/{epochs}")
        print(f'Epoch: {epoch}, Batch: {i+1}, Loss: {loss.item():.4f}')

        if epoch % 10 == 0:
            score = 0
            ssim_score = 0
            rmse_score = 0
            total_diff = 0
            validation_loss = 0  # Initialize validation loss
            model.eval()
            with torch.no_grad():
                for i, (input_image, output_image, source_image, target_image, _, _) in enumerate(valid_loader):
                    
                    input_image = input_image.to(device)
                    output_image = output_image.to(device)
                    source_image = source_image.to(device)
                    target_image = target_image.to(device)
                    
                    output = model(input_image, source_image, target_image)

                    
                    val_loss = criterion(output, output_image)  # Calculate validation loss
                    validation_loss += val_loss.item()

                    output = output[0].detach().cpu().numpy().transpose(1, 2, 0)
                    output = output.clip(0, 1)
                    output = (output * 255.0).astype(np.uint8)

                    input_image = input_image[0].detach().cpu().numpy().transpose(1, 2, 0)
                    input_image = input_image.clip(0, 1)
                    input_image = (input_image * 255.0).astype(np.uint8)
                    
                    output_image = output_image[0].detach().cpu().numpy().transpose(1, 2, 0)
                    output_image = output_image.clip(0, 1)
                    output_image = (output_image * 255.0).astype(np.uint8)
                    
                    diff = test(Image.fromarray(output_image), Image.fromarray(output))
                    total_diff = total_diff + diff
                    iou = calculate_iou_for_single_image(Image.fromarray(output_image), Image.fromarray(output))
                    score = score + iou
                    ssim = calculate_ssim_for_single_image(Image.fromarray(output_image), Image.fromarray(output))
                    ssim_score = ssim_score + ssim
                    rmse = calculate_rmse_for_single_image(Image.fromarray(output_image), Image.fromarray(output))
                    rmse_score = rmse_score + rmse

            validation_loss /= len(valid_loader)
            score = score / len(valid_loader)
            ssim_score = ssim_score / len(valid_loader)
            rmse_score = rmse_score / len(valid_loader)
            total_diff = total_diff / len(valid_loader)
            print("IOU score after: " + str(epoch) + " epochs: " + str(score))
            print("SSIM score after: " + str(epoch) + " epochs: " + str(ssim_score))
            print("RMSE score after: " + str(epoch) + " epochs: " + str(rmse_score))
            print("Pixel Diff score after: " + str(epoch) + " epochs: " + str(total_diff))

            if validation_loss < best_loss - min_delta:
                best_loss = validation_loss
                epochs_without_improvement = 0
                # Optionally save the best model here
                torch.save(model.state_dict(), model_path)

            else:
                epochs_without_improvement += 1
                if epochs_without_improvement >= patience:
                    print("Early stopping triggered")
                    break  # Stop training
                
    torch.save(model.state_dict(), model_path)

    print("### Model Summary ###")
    model_size_MB, param_count = get_model_size(model)
    print(f"Model Parameter Size: {model_size_MB:.2f} MB, {param_count} parameters")

    # Inference
    model.eval()
    print("Saving output.")
    with torch.no_grad():
        
        score = 0
        ssim_score = 0
        rmse_score = 0
        total_diff = 0
        validation_loss = 0  
        for i, (input_image, output_image, source_image, target_image, input_image_filepath, output_image_filepath) in enumerate(train_loader2):
            # print(input_image_filepath[0])

            filename = input_image_filepath[0].split('/')[-1]
            # print(filename)
            number_str = filename.split('.')[0]
            prefix, suffix = number_str.split('_', 1)
            # number = int(number_str)
            # print(number)

            input_image = input_image.to(device)
            output_image = output_image.to(device)
            source_image = source_image.to(device)
            target_image = target_image.to(device)
            
            # print(output_image.shape)
            output = model(input_image, source_image, target_image)
            
            
            output = output[0].detach().cpu().numpy().transpose(1, 2, 0)
            # Unnormalize the image
            mean = np.array([0.5, 0.5, 0.5])
            std = np.array([0.5, 0.5, 0.5])
            output = output * std + mean
            
            output = output.clip(0, 1)
            output = (output * 255.0).astype(np.uint8)

            diff = test(Image.fromarray(output_image), Image.fromarray(output))
            total_diff = total_diff + diff
            iou = calculate_iou_for_single_image(Image.fromarray(output_image), Image.fromarray(output))
            score = score + iou
            ssim = calculate_ssim_for_single_image(Image.fromarray(output_image), Image.fromarray(output))
            ssim_score = ssim_score + ssim
            rmse = calculate_rmse_for_single_image(Image.fromarray(output_image), Image.fromarray(output))
            rmse_score = rmse_score + rmse

            plt.imshow(output)
            plt.axis('off')
            plt.title('')
            # Save the figure
            file_path = train_output_image_paths_root
            output_path = f"{file_path}/{(prefix)}_{(suffix)}.png"
            plt.savefig(output_path, bbox_inches='tight', pad_inches=0)
            if(i%100) == 0:
                print(output_path)

        validation_loss /= len(train_loader2)
        score = score / len(train_loader2)
        ssim_score = ssim_score / len(train_loader2)
        rmse_score = rmse_score / len(train_loader2)
        total_diff = total_diff / len(train_loader2)
        print("IOU score after: " + str(epoch) + " epochs: " + str(score))
        print("SSIM score after: " + str(epoch) + " epochs: " + str(ssim_score))
        print("RMSE score after: " + str(epoch) + " epochs: " + str(rmse_score))
        print("Pixel Diff score after: " + str(epoch) + " epochs: " + str(total_diff))

        score = 0
        ssim_score = 0
        rmse_score = 0
        total_diff = 0
        validation_loss = 0  
        
        for i, (input_image, output_image, source_image, target_image, input_image_filepath, output_image_filepath) in enumerate(valid_loader2):
            filename = input_image_filepath[0].split('/')[-1]
            number_str = filename.split('.')[0]
            prefix, suffix = number_str.split('_', 1)
            # number = (number_str)

            input_image = input_image.to(device)
            output_image = output_image.to(device)
            source_image = source_image.to(device)
            target_image = target_image.to(device)
            
            # print(output_image.shape)
            output = model(input_image, source_image, target_image)
            
            
            output = output[0].detach().cpu().numpy().transpose(1, 2, 0)

            mean = np.array([0.5, 0.5, 0.5])
            std = np.array([0.5, 0.5, 0.5])
            output = output * std + mean

            output = output.clip(0, 1)
            output = (output * 255.0).astype(np.uint8)
            
            diff = test(Image.fromarray(output_image), Image.fromarray(output))
            total_diff = total_diff + diff
            iou = calculate_iou_for_single_image(Image.fromarray(output_image), Image.fromarray(output))
            score = score + iou
            ssim = calculate_ssim_for_single_image(Image.fromarray(output_image), Image.fromarray(output))
            ssim_score = ssim_score + ssim
            rmse = calculate_rmse_for_single_image(Image.fromarray(output_image), Image.fromarray(output))
            rmse_score = rmse_score + rmse

            plt.imshow(output)
            plt.axis('off')

            # Save the figure
            file_path = test_output_image_paths_root
            output_path = f"{file_path}/{(prefix)}_{(suffix)}.png"
            plt.savefig(output_path, bbox_inches='tight', pad_inches=0)
            if(i%10) == 0:
                print(output_path)

        validation_loss /= len(valid_loader2)
        score = score / len(valid_loader2)
        ssim_score = ssim_score / len(valid_loader2)
        rmse_score = rmse_score / len(valid_loader2)
        total_diff = total_diff / len(valid_loader2)
        print("IOU score after: " + str(epoch) + " epochs: " + str(score))
        print("SSIM score after: " + str(epoch) + " epochs: " + str(ssim_score))
        print("RMSE score after: " + str(epoch) + " epochs: " + str(rmse_score))
        print("Pixel Diff score after: " + str(epoch) + " epochs: " + str(total_diff))

if __name__ == '__main__':
    main()

