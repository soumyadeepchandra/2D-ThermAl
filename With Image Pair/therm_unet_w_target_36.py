# -*- coding: utf-8 -*-
"""Therm_UNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V7uuS3JbE0Nianw8ySQ619r2zV66qeqG
"""

import os
import numpy as np # linear algebra
import time
from contextlib import contextmanager # timer
from functools import partial
import glob
# import seaborn as sns

import matplotlib.pylab as plt

# from skimage.transform import rescale, resize
import torch
import torch.nn as nn

#from torch.utils import data
from torch.utils.data import DataLoader, Dataset
import albumentations as A  #####


import torch.nn.functional as F
import argparse
import math
import os.path
import shutil

import scipy.io
import torch.nn.parallel

from torch.autograd import Variable
import torch.optim

from PIL import Image
import random
import torchvision.transforms as transforms

if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

# transform_train = A.Compose([
#     A.Rotate(limit=45, p=0.5),
#     A.HorizontalFlip(p=0.5),
#     A.Flip(p=0.5),
#     A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.2),
#     A.RandomCrop(width=200, height=200, p=0.5),
#     transforms.Resize((224, 224)),
#     transforms.ToTensor(),
#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
# ])

transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

albumentations_transform = A.Compose([
    A.Rotate(limit=45, p=0.5),
    A.HorizontalFlip(p=0.5),
    A.Flip(p=0.5),
    #A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.2),
    #A.RandomCrop(width=200, height=200, p=0.5),
], additional_targets={"image4": "image", "image3": "image", "image2": "image"})

transform_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

class ThermDataset(Dataset):
    def __init__(self, input_image_paths, gt_image_paths, transform=False, study = 'train'): #transform = true in train, false in test
        self.input_image_paths = input_image_paths
        self.gt_image_paths = gt_image_paths
        self.transform = transform
        self.study = study
        # print(self.input_image_paths)

    def __len__(self):
        return len(self.input_image_paths)

    #idx is handled auto, u dont need to specify
    def __getitem__(self, idx):

        input_image_filepath = self.input_image_paths[idx]
        output_image_filepath = self.gt_image_paths[idx]
        image_input = Image.open(input_image_filepath).convert("RGB")
        image_output = Image.open(output_image_filepath).convert("RGB")
        image_input = image_input.resize((224,224))
        image_output = image_output.resize((224,224))

        dataset_length = len(self)
        random_idx = random.choice([i for i in range(dataset_length) if i != idx])
        source_image_filepath = self.input_image_paths[random_idx]
        target_image_filepath = self.gt_image_paths[random_idx]
        image_source = Image.open(source_image_filepath).convert("RGB")
        image_source = image_source.resize((224,224))
        image_target = Image.open(target_image_filepath).convert("RGB")
        image_target = image_target.resize((224,224))


        if self.study == 'train':
            image_np = np.array(image_input)
            image_np2 = np.array(image_output)
            image_np3 = np.array(image_source)
            image_np4 = np.array(image_target)
            augmented = albumentations_transform(image=image_np, image2=image_np2, image3=image_np3, image4= image_np4)
            image_np_transformed = augmented['image']
            image_input = Image.fromarray(image_np_transformed)
            image_np_transformed2 = augmented['image2']
            image_output = Image.fromarray(image_np_transformed2)
            image_np_transformed3 = augmented['image3']
            image_source = Image.fromarray(image_np_transformed3)
            image_np_transformed4 = augmented['image4']
            image_target = Image.fromarray(image_np_transformed4)


        # Both input and output are transformed
        if self.transform is not None:
            image_input = self.transform(image_input)
            image_output = self.transform(image_output)
            image_source = self.transform(image_source)
            image_target = self.transform(image_target)

        return image_input, image_output, image_source, image_target, input_image_filepath, output_image_filepath

class ThermDataset2(Dataset):
    def __init__(self, input_image_paths, gt_image_paths, transform=False, study = 'train'): #transform = true in train, false in test
        self.input_image_paths = input_image_paths
        self.gt_image_paths = gt_image_paths
        self.transform = transform
        self.study = study
        # print(self.input_image_paths)

    def __len__(self):
        return len(self.input_image_paths)

    #idx is handled auto, u dont need to specify
    def __getitem__(self, idx):
        input_image_filepath = self.input_image_paths[idx]
        output_image_filepath = self.gt_image_paths[idx]
        image_input = Image.open(input_image_filepath).convert("RGB")
        image_output = Image.open(output_image_filepath).convert("RGB")
        image_input = image_input.resize((224,224))
        image_output = image_output.resize((224,224))
        
        dataset_length = len(self)
        random_idx = random.choice([i for i in range(dataset_length) if i != idx])
        source_image_filepath = self.input_image_paths[random_idx]
        target_image_filepath = self.gt_image_paths[random_idx]
        image_source = Image.open(source_image_filepath).convert("RGB")
        image_source = image_source.resize((224,224))
        image_target = Image.open(target_image_filepath).convert("RGB")
        image_target = image_target.resize((224,224))


        # Both input and output are transformed
        if self.transform is not None:
            image_input = self.transform(image_input)
            image_output = self.transform(image_output)
            image_source = self.transform(image_source)
            image_target = self.transform(image_target)

        return image_input, image_output, image_source, image_target, input_image_filepath, output_image_filepath



def double_conv2d(in_channel, out_channel):
    """
    (convolution => [BN] => ReLU) * 2
    """
    convLayer = nn.Sequential(
        nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1),
        nn.BatchNorm2d(out_channel),
        nn.ReLU(inplace=True),
        nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1),
        nn.BatchNorm2d(out_channel),
        nn.ReLU(inplace=True)
    )
    return convLayer

class PositionalEmbedding(nn.Module):
    def __init__(self, num_channels, height, width):
        super(PositionalEmbedding, self).__init__()
        self.height = height
        self.width = width
        self.num_channels = num_channels
        # Create a learnable positional embedding
        self.embedding = nn.Parameter(torch.randn(1, num_channels, height, width))

    def forward(self, x):
        # Add the positional embedding to the input
        # Resize positional embedding to match the input tensor's spatial dimensions
        embedding_resized = nn.functional.interpolate(self.embedding, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)

        # If the input tensor has more channels (e.g., due to concatenation), repeat the embedding along the channel dimension
        if x.shape[1] != self.num_channels:
            embedding_resized = embedding_resized.repeat(1, x.shape[1] // self.num_channels, 1, 1)

        return x + embedding_resized

class DiffUnet(nn.Module): 
    def __init__(self, height, width):
        super(DiffUnet, self).__init__()
        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)

        # Encoder layers
        self.downConvLayer_1 = double_conv2d(3, 48)
        self.downConvLayer_2 = double_conv2d(48, 96)
        self.downConvLayer_3 = double_conv2d(96, 192)
        self.downConvLayer_4 = double_conv2d(192, 384)
        self.downConvLayer_5 = double_conv2d(384, 768)
        
        # Decoder layers with transposed convolutions
        self.upTransConv1 = nn.ConvTranspose2d(in_channels=768*3, out_channels=384, kernel_size=(2, 2), stride=(2, 2))
        self.upConvLayer_1 = double_conv2d(768 + 384*2, 384)

        self.upTransConv2 = nn.ConvTranspose2d(in_channels=384, out_channels=192, kernel_size=(2, 2), stride=(2, 2))
        self.upConvLayer_2 = double_conv2d(384 + 192*2, 192)

        self.upTransConv3 = nn.ConvTranspose2d(in_channels=192, out_channels=96, kernel_size=(2, 2), stride=(2, 2))
        self.upConvLayer_3 = double_conv2d(192 + 96*2, 96)

        self.upTransConv4 = nn.ConvTranspose2d(in_channels=96, out_channels=48, kernel_size=(2, 2), stride=(2, 2))
        self.upConvLayer_4 = double_conv2d(96 + 48*2, 48)

        self.out = nn.Conv2d(in_channels=48, out_channels=3, kernel_size=(1, 1)) 

        # Positional embedding
        self.positional_embedding = PositionalEmbedding(num_channels=768, height=height//32, width=width//32)

    def encode(self, image):
        # Encoder
        d1_out = self.downConvLayer_1(image)
        d2_out = self.downConvLayer_2(self.max_pool(d1_out))
        d3_out = self.downConvLayer_3(self.max_pool(d2_out))
        d4_out = self.downConvLayer_4(self.max_pool(d3_out))
        d5_out = self.downConvLayer_5(self.max_pool(d4_out))
        return d1_out, d2_out, d3_out, d4_out, d5_out

    def forward(self, image_input, image_source, image_target):
        """
        image_input shape: batch_size, channel, height, width
        image_source shape: batch_size, channel, height, width
        image_target shape: batch_size, channel, height, width
        """

        # Encoding: encode image_input, image_source, and image_target
        d1_in, d2_in, d3_in, d4_in, d5_in = self.encode(image_input)
        d1_source, d2_source, d3_source, d4_source, d5_source = self.encode(image_source)
        d1_target, d2_target, d3_target, d4_target, d5_target = self.encode(image_target)
        

        # Fusion: concatenate the encoded features from the three inputs
        d5_fused = torch.cat([d5_in, d5_source, d5_target], dim=1)

        # Add positional embedding to the fused features
        d5_fused_with_pos = self.positional_embedding(d5_fused)

        """
        Decoder: Decode the fused and positionally embedded features
        """
        up_sampling1 = self.upTransConv1(d5_fused_with_pos)
        u1_out = self.upConvLayer_1(torch.cat([d4_in, d4_source, d4_target, up_sampling1], axis=1))

        up_sampling2 = self.upTransConv2(u1_out)
        u2_out = self.upConvLayer_2(torch.cat([d3_in, d3_source, d3_target, up_sampling2], axis=1))

        up_sampling3 = self.upTransConv3(u2_out)
        u3_out = self.upConvLayer_3(torch.cat([d2_in, d2_source, d2_target, up_sampling3], axis=1))

        up_sampling4 = self.upTransConv4(u3_out)
        u4_out = self.upConvLayer_4(torch.cat([d1_in, d1_source, d1_target, up_sampling4], axis=1))

        # Final output layer
        final_out = self.out(u4_out)

        return final_out

def test(image_path, output_path):
    # image = Image.open(image_path).convert("RGB")
    # output = Image.open(output_path).convert("RGB")

    # Convert images to numpy arrays
    image1_array = np.array(image_path)
    image2_array = np.array(output_path)

    diff = 0
    # Check if the dimensions are the same
    if image1_array.shape == image2_array.shape:
        # Compute the difference
        difference = np.abs(image1_array - image2_array)
        # print(difference.shape)
        diff = np.sum(difference)/(224*224*3)

    return diff

def calculate_iou_for_single_image(input_image, output_image):
  # Calculate the intersection and union of the two images
  intersection = np.logical_and(input_image, output_image)
  union = np.logical_or(input_image, output_image)

  # Calculate the IOU
  iou = np.sum(intersection) / np.sum(union)

  return iou

def get_model_size(model):
    param_size = 0
    param_count = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
        param_count += param.nelement()
    param_size_MB = param_size / (1024 ** 2)
    return param_size_MB, param_count

# 001->003->006->010->020->041
############################# Stage 2 -> 003 to 006 #######################################
train_input_image_paths_root = F"/home/nano01/a/chand133/ThermAI/Dataset/Train/Input/003/"
train_output1_image_paths_root = F"/home/nano01/a/chand133/ThermAI/Dataset/Train/Output_target/006/"
train_gt_image_paths_root = F"/home/nano01/a/chand133/ThermAI/Dataset/Train/Input/006/"

test_input_image_paths_root = F"/home/nano01/a/chand133/ThermAI/Dataset/Test/Input/003/"
test_output1_image_paths_root = F"/home/nano01/a/chand133/ThermAI/Dataset/Test/Output_target/006/"
test_gt_image_paths_root = F"/home/nano01/a/chand133/ThermAI/Dataset/Test/Input/006/"

# Load the updated model state dictionary into the model
model_path = "/home/nano01/a/chand133/ThermAI/stage_target_003_to_006.pth"

if not os.path.exists(train_output1_image_paths_root):
    # print(img_path)
    os.makedirs(train_output1_image_paths_root)

if not os.path.exists(test_output1_image_paths_root):
    # print(img_path)
    os.makedirs(test_output1_image_paths_root)

def flatten(l): return flatten(l[0]) + (flatten(l[1:]) if len(l) > 1 else []) if type(l) is list else [l]

train_input_image_paths = [] #to store image paths in list
train_gt_image_paths = []

for data_path in glob.glob(train_input_image_paths_root + '/*'):
    train_input_image_paths.append(data_path)

for data_path in glob.glob(train_gt_image_paths_root + '/*'):
    train_gt_image_paths.append(data_path)

train_input_image_paths = sorted(list(flatten(train_input_image_paths)))
train_gt_image_paths = sorted(list(flatten(train_gt_image_paths)))

print('train_image_path example: ', train_input_image_paths[0])

test_input_image_paths = [] #to store image paths in list
test_gt_image_paths = []

for data_path in glob.glob(test_input_image_paths_root + '/*'):
    test_input_image_paths.append(data_path)

for data_path in glob.glob(test_gt_image_paths_root + '/*'):
    test_gt_image_paths.append(data_path)

test_input_image_paths = sorted(list(flatten(test_input_image_paths)))
test_gt_image_paths = sorted(list(flatten(test_gt_image_paths)))


print('test_image_path example: ', test_input_image_paths[0])
# print('test_image_path example: ', test_gt_image_paths)


epochs= 2000
learning_rate= 1e-6
save_checkpoint= True

amp= False
weight_decay= 1e-8
momentum= 0.999
gradient_clipping= 1.0


# Training
train_dataset = ThermDataset(train_input_image_paths, train_gt_image_paths, transform_train, study = 'train')
valid_dataset = ThermDataset(test_input_image_paths, test_gt_image_paths, transform_test, study = 'test')


batch_size = 75
train_loader = DataLoader(
    train_dataset, batch_size=batch_size, shuffle=True
)

valid_loader = DataLoader(
    valid_dataset, batch_size=4, shuffle=False
)

# Inference
train_dataset = ThermDataset2(train_input_image_paths, train_gt_image_paths,transform_train, study = 'train')
valid_dataset = ThermDataset2(test_input_image_paths, test_gt_image_paths,transform_test, study = 'test')

batch_size = 1


train_loader2 = DataLoader(
    train_dataset, batch_size=batch_size, shuffle=True
)

valid_loader2 = DataLoader(
    valid_dataset, batch_size=batch_size, shuffle=False
)

def main():
    loss_fn = torch.nn.BCEWithLogitsLoss()
    scaler = torch.cuda.amp.GradScaler()
    model = DiffUnet(224,224)
    
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path))
        print("Model Loaded Successfully!")

    # move initialised model to chosen device
    model = model.to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    patience = 10  # Number of epochs to wait before stopping after last improvement
    min_delta = 0.01  # Minimum change in the monitored quantity to qualify as an improvement
    best_loss = float('inf')
    epochs_without_improvement = 0

    # Training
    for epoch in range(1, epochs+1):
        # print(f"Training epoch {epoch}/{epochs}")
        for i, (input_image, output_image, source_image, target_image, _, _) in enumerate(train_loader):
            input_image = input_image.to(device)
            output_image = output_image.to(device)
            source_image = source_image.to(device)
            target_image = target_image.to(device)
            
            # print(output_image.shape)
            output = model(input_image, source_image, target_image)

            loss = criterion(output, output_image)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        if epoch % 10 == 0:
            print(f"Training epoch {epoch}/{epochs}")
            print(f'Epoch: {epoch}, Batch: {i+1}, Loss: {loss.item():.4f}')

        if epoch % 100 == 0:
            score = 0
            total_diff = 0
            validation_loss = 0
            model.eval()
            with torch.no_grad():
                for i, (input_image, output_image, source_image, target_image, _, _) in enumerate(valid_loader):
                    
                    input_image = input_image.to(device)
                    output_image = output_image.to(device)
                    source_image = source_image.to(device)
                    target_image = target_image.to(device)
                    
                    output = model(input_image, source_image, target_image)

                    val_loss = criterion(output, output_image)  # Calculate validation loss
                    validation_loss += val_loss.item()

                    output = output[0].detach().cpu().numpy().transpose(1, 2, 0)
                    output = output.clip(0, 1)
                    output = (output * 255.0).astype(np.uint8)

                    input_image = input_image[0].detach().cpu().numpy().transpose(1, 2, 0)
                    input_image = input_image.clip(0, 1)
                    input_image = (input_image * 255.0).astype(np.uint8)
                    
                    output_image = output_image[0].detach().cpu().numpy().transpose(1, 2, 0)
                    output_image = output_image.clip(0, 1)
                    output_image = (output_image * 255.0).astype(np.uint8)
                    
                    diff = test(Image.fromarray(output_image), Image.fromarray(output))
                    total_diff = total_diff + diff
                    iou = calculate_iou_for_single_image(Image.fromarray(input_image), Image.fromarray(output))
                    score = score + iou

            validation_loss /= len(valid_loader)
            score /= len(valid_loader)
            total_diff /= len(valid_loader)

            print(f"Validation Loss after {epoch} epochs: {validation_loss:.4f}")
            print(f"IOU score after {epoch} epochs: {score}")
            print(f"Pixel Diff score after {epoch} epochs: {total_diff}")

            # Check for improvement
            if validation_loss < best_loss - min_delta:
                best_loss = validation_loss
                epochs_without_improvement = 0
                # Optionally save the best model here
                torch.save(model.state_dict(), model_path)

            else:
                epochs_without_improvement += 1
                if epochs_without_improvement >= patience:
                    print("Early stopping triggered")
                    break  # Stop training

    torch.save(model.state_dict(), model_path)

    print("### Model Summary ###")
    model_size_MB, param_count = get_model_size(model)
    print(f"Model Parameter Size: {model_size_MB:.2f} MB, {param_count} parameters")

    # Inference
    model.eval()
    print("Saving output.")
    with torch.no_grad():
        for i, (input_image, output_image, source_image, target_image, input_image_filepath, output_image_filepath) in enumerate(train_loader2):
            # print(input_image_filepath[0])

            filename = input_image_filepath[0].split('/')[-1]
            # print(filename)
            number_str = filename.split('.')[0]
            prefix,suffix = number_str.split('_')
            # print(number)

            input_image = input_image.to(device)
            output_image = output_image.to(device)
            source_image = source_image.to(device)
            target_image = target_image.to(device)
            
            # print(output_image.shape)
            output = model(input_image, source_image, target_image)
            
            
            output = output[0].detach().cpu().numpy().transpose(1, 2, 0)
            # Unnormalize the image
            mean = np.array([0.5, 0.5, 0.5])
            std = np.array([0.5, 0.5, 0.5])
            output = output * std + mean
            
            output = output.clip(0, 1)
            output = (output * 255.0).astype(np.uint8)

            plt.imshow(output)
            plt.axis('off')
            plt.title('')
            # Save the figure
            output_path = f"{train_output1_image_paths_root}{prefix}_006.png"
            plt.savefig(output_path, bbox_inches='tight', pad_inches=0)
            if(i%100) == 0:
                print(output_path)

        for i, (input_image, output_image, source_image, target_image, input_image_filepath, output_image_filepath) in enumerate(valid_loader2):
            filename = input_image_filepath[0].split('/')[-1]
            number_str = filename.split('.')[0]
            prefix,suffix = number_str.split('_')

            input_image = input_image.to(device)
            output_image = output_image.to(device)
            source_image = source_image.to(device)
            target_image = target_image.to(device)
            
            # print(output_image.shape)
            output = model(input_image, source_image, target_image)
            
            
            output = output[0].detach().cpu().numpy().transpose(1, 2, 0)

            mean = np.array([0.5, 0.5, 0.5])
            std = np.array([0.5, 0.5, 0.5])
            output = output * std + mean

            output = output.clip(0, 1)
            output = (output * 255.0).astype(np.uint8)
            
            plt.imshow(output)
            plt.axis('off')

            # Save the figure
            output_path = f"{test_output1_image_paths_root}{prefix}_006.png"
            plt.savefig(output_path, bbox_inches='tight', pad_inches=0)
            if(i%10) == 0:
                print(output_path)



if __name__ == '__main__':
    main()

